from openai import OpenAI
import streamlit as st

st.title("ğŸ‘©â€ğŸ«Yoonie Teacher's chatbot")

client = OpenAI()

# ë§íˆ¬ ì„ íƒ
tone = st.radio("ì›í•˜ëŠ” ë§íˆ¬ë¥¼ ì„ íƒí•˜ì„¸ìš” ğŸ‘‡", ("ë†’ì„ë§", "ë°˜ë§"), horizontal=True)

# ëª¨ë¸ ì„¤ì •
if "openai_model" not in st.session_state:
    st.session_state["openai_model"] = "gpt-4.1"

# ë©”ì‹œì§€ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ëŒ€í™” ê¸°ë¡ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if prompt := st.chat_input("ì§€ê¸ˆ ê¸°ë¶„ì€ ì–´ë•Œ?"):

    # ì…ë ¥ ì €ì¥
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    # ë§íˆ¬ì— ë”°ë¼ system prompt ì„¤ì •
    if tone == "ë†’ì„ë§":
        system_prompt = {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ì´ˆë“±í•™êµ ì„ ìƒë‹˜ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê³µì†í•˜ê³  ì˜ˆì˜ ë°”ë¥´ê²Œ, ë†’ì„ë§ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        }
    else:  # ë°˜ë§
        system_prompt = {
            "role": "system",
            "content": "ë„ˆëŠ” ì´ˆë“±í•™ìƒ ì¹œêµ¬ë‘ ì´ì•¼ê¸°í•˜ë“¯ ì¹œê·¼í•˜ê²Œ ë°˜ë§ë¡œ ëŒ€ë‹µí•´. ë„ˆë¬´ ë”±ë”±í•˜ì§€ ì•Šê²Œ, ì¬ë¯¸ìˆê³  í¸í•˜ê²Œ ë§í•´ì¤˜."
        }

    with st.chat_message("assistant"):
        full_response = ""

        stream = client.chat.completions.create(
            model=st.session_state["openai_model"],
            messages=[system_prompt] + [
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            stream=True,
        )

        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                st.write(content, end="")

        st.session_state.messages.append({"role": "assistant", "content": full_response})
